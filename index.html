<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <meta name="description" content="CDx">
  <meta property="og:title" content="DiffuseKronA"/>
  <meta property="og:description" content="A Parameter Efficient Fine-tuning Method for Personalized
  Diffusion Models"/>
  <meta property="og:url" content="diffusekrona.github.io"/>

  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="DiffuseKronA, Personalized Text-to-image generation, Diffusion, LDM, Stable Diffusion, SDXL, Kronecker Product, LoRA">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <link rel="icon" type="image/x-icon" href="static/images/CDx-Logo.png">
  <link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Hedvig+Letters+Serif:opsz@12..24&display=swap" rel="stylesheet">
    <title>DiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized
      Diffusion Models</title>
    
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <style>
    .gallery2 {
        display: flex;
        justify-content: flex-start;
        align-items: center;
        overflow-x: auto;
        white-space: nowrap;
        border: 2px solid #000;
        border-radius: 5px;
        margin: 20px;
        padding: 10px;
    }
    .gallery2 .image-container {
        display: flex;
        flex-direction: column;
        align-items: center;
        margin: 0 5px;
    }
        .gallery2 img {
            margin: 0 5px;
            border: 2px solid #000;
            border-radius: 5px;
            box-shadow: 0 0 10px rgba(0,0,0,0.5);
        }

        ::-webkit-scrollbar {
        width: 5px;
    }

    /* Track */
    ::-webkit-scrollbar-track {
        background: #f1f1f1; 
    }
    
    /* Handle */
    ::-webkit-scrollbar-thumb {
        background: #888; 
    }

    /* Handle on hover */
    ::-webkit-scrollbar-thumb:hover {
        background: #555; 
    }

  .dataset {
    display: flex;
    justify-content: center;
    align-items: center;
    flex-wrap: nowrap;
}

.dataset img {
    margin: 0 5px;
    border: 2px solid #000;
    border-radius: 5px;
    box-shadow: 0 0 10px rgba(0,0,0,0.5);
}

.caption {
    text-align: center;
    border: 2px solid #000;
    border-radius: 5px;
    padding: 10px;
    margin-top: 10px;
    margin-bottom: 0;
    box-shadow: 0 0 10px rgba(0,0,0,0.5);
    font-style: italic;
}
.scroll-box {
  height: 700px; /* Adjust as needed */
  overflow-y: auto;
}

table {
        border-collapse: collapse;
        width: 100%;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(odd) {
        background-color: #f2f2f2;
    }
    th {
        padding-top: 12px;
        padding-bottom: 12px;
        text-align: left;
        background-color: #4CAF50;
        color: white;
    }
  </style>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">DiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized
              Diffusion Model</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/shyammarjit" target="_blank">Shyam Marjit</a><sup>*,1</sup> ,</span>
                <span class="author-block">

                  <a href="https://iamdata.tech/" target="_blank">Harshit Singh</a><sup>*,1</sup> ,</span>
                  <span class="author-block">

                <a href="https://nityanandmathur.live" target="_blank">Nityanand Mathur</a><sup>*,1</sup> ,</span>
                <span class="author-block">

                    <a href="https://sayak.dev/" target="_blank">Sayak Paul</a><sup>2</sup> ,
                  </span>
                  <span class="author-block">
                    <a href="http://chiamuyu.weebly.com/" target="_blank">Chia-Mu Yu</a><sup>3</sup> ,
                  </span>

                  <span class="author-block">
                    <a href="https://sites.google.com/site/pinyuchenpage/home?authuser=0" target="_blank">Pin-Yu Chen</a><sup>4</sup>
                  </span>

                  </div>  

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>IIIT Guwahati, <sup>2</sup>Hugging Face, <sup>3</sup>National Yang Ming Chiao Tung University,
                     <sup>4</sup>IBM Research </span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>
                  
                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2312.02345" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper(soon)</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/IBM/DiffuseKronA" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code(Soon)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2312.02345" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>

                <span class="link-block">
                  <a href="gallery.html" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-play"></i>
                  </span>
                  <span>Gallery</span>
                </a>
              </span>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<hr class="divider" />
<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls loop muted height="100%" style="max-width: 100%;">
        <!-- Your video here -->
        <source src="static/images/DiffuseKronA.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <div class="content has-text-justified">
          Our method, DiffuseKronA, achieves superior image quality and accurate text-image correspondence across diverse input images
          and prompts, all the while upholding exceptional parameter efficiency. In this context, \([V]\) denotes a unique token used for fine-tuning a
          specific subject in the text-to-image diffusion model.
        </div>
      </h2> 
    </div>
  </div>
</section>
<!-- End teaser video -->
<hr class="divider" />
<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In the realm of subject-driven text-to-image (T2I) generative models, recent developments like DreamBooth and BLIP-Diffusion have led to impressive results yet encounter limitations due to their intensive fine-tuning demands and substantial parameter requirements. While the low-rank adaptation (LoRA) module within DreamBooth offers a reduction in trainable parameters, it introduces a pronounced sensitivity to hyperparameters, leading to a compromise between parameter efficiency and the quality of T2I personalized image synthesis. Addressing these constraints, we introduce DiffuseKronA, a novel Kronecker product-based adaptation module that not only significantly reduces the parameter count by up to <em>35%</em> and <em>99.947%</em> compared to LoRA-DreamBooth and the original DreamBooth, respectively, but also enhances the quality of image synthesis. Crucially, DiffuseKronA mitigates the issue of hyperparameter sensitivity, delivering consistent high-quality generations across a wide range of hyperparameters, thereby diminishing the necessity for extensive fine-tuning. Evaluated against diverse and complex input images and text prompts, DiffuseKronA consistently outperforms existing models, producing diverse images of higher quality with improved fidelity and a more accurate color distribution of objects, thus presenting a substantial advancement in the field of T2I generative modeling.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<hr class="divider">
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="section-title has-text-centered is-centered">
      <h2 class="title is-3">Generating your dream images from text</h2><br>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column  has-text-centered">
          <img id="network" src="static/images/model_dig.png" style="max-width: 100%;"/>
          <div class="content has-text-justified">
            The main idea of DiffuseKronA is to use the Kronecker product, a matrix multiplication method that captures pairwise interactions between elements of two matrices, to decompose the weight matrices of the attention layers in the UNet model.
            Kronecker Product is a matrix multiplication method that captures structured relationships and pairwise interactions between elements of two matrices, offering a higher-rank approximation with less parameter count and greater flexibility.
            Kronecker Adapter replaces the low-rank decomposition in LoRA with the Kronecker product, such that \(W_{\text{pre-trained}}+\Delta W = W_{\text{pre-trained}} + (A \otimes B)\), where A and B are the Kronecker factors, and ⊗ denotes the Kronecker product.
            Kronecker Adapter reduces the computational cost by using the following equivalent matrix-vector multiplication: \(    (A \otimes B) x=\gamma\left(B \eta_{b_2 \times a_2}(x) A^{\top}\right)\), where vec is the vectorization operator, and T is the transpose operator.
            <br>
            <div style="border: 1px solid black; padding: 1px 10px; margin: 4px auto; display: table;">
              $$W_{\text{fine-tuned}}=W_{\text{pre-trained}}+\Delta W$$
            </div>
            <div style="border: 1px solid black; padding: 1px 10px; margin: 4px auto; display: table;">
              $$\Delta W^{U} =A^U \otimes \mathbf{B}^U$$
            </div>
            <div style="border: 1px solid black; padding: 1px 10px; margin: 4px auto; display: table;">
              $$    A \otimes B=\left[\begin{array}{ccc}
              a_{1,1} B & \cdots & a_{1,a_2} B \\
              \vdots & \ddots & \vdots \\
              a_{a_1, 1} B & \cdots & a_{a_1, a_2} B
              \end{array}\right]$$
            </div>
          </div>
      </div>      
  </div>
  </section>

<hr class="divider" />  

<section class="section">
  <div class="container is-widescreen">
    <div class="section-title has-text-centered is-centered">
      <h2 class="title is-3">Unraveling Textual Descriptions into Artistic Creations</h2><br>
    </div>

<div class="scroll-box">
      <div class="dataset">
        <img src="./static/vase/input/1.png" style="width: 224px;">
        <img src="./static/vase/input/2.png" style="width: 224px;">
        <img src="./static/vase/input/3.png" style="width: 224px;">
        <img src="./static/vase/input/4.png" style="width: 224px;">
      </div>
      <div style="display: flex; flex-direction: column; align-items: center;">
        <p class="caption" style="width: 20%;">A [V] Vase</p>
      </div>
    
    <div id="gallery" class="gallery2">
      <!-- Images will be inserted here by JavaScript -->
    </div>

    <script src="./static/js/gallery.js"></script>

    <script>
      var images = ['a [V] vase in front of Eiffel Tower.png', 'a [V] transparent vase with milk inside.png', 'a [V] vase buried in sand.png', 'a [V] vase floating on water.png', 'a [V] vase in beach.png', 'a [V] vase in cobblestone street.png', 'a [V] vase in snow.png', 'a [V] vase of green colour.png', 'A [V] vase on a road in city.png', 'a [V] vase with autumn leaves inside.png', 'a [V] vase with mountains in background.png', 'two [V] vases on a table.png'];
      populateGallery(images,'vase','gallery');
    </script> 

<hr class="divider">

<div class="dataset">
  <img src="./static/results/accordion.png" style="width: 224px;">
  <img src="./static/results/accordion.png" style="width: 224px;">
  <img src="./static/results/accordion.png" style="width: 224px;">
  <img src="./static/results/accordion.png" style="width: 224px;">
</div>
<div style="display: flex; flex-direction: column; align-items: center;">
  <p class="caption" style="width: 20%;">Your caption goes here</p>
</div>

<div id="gallery2" class="gallery2">
<!-- Images will be inserted here by JavaScript -->
</div>

<script src="./static/js/gallery.js"></script>

<script>
var images = ["accordion.png", "accordion.png", "accordion.png", "accordion.png", "accordion.png", "accordion.png","accordion.png","accordion.png"];
populateGallery(images, 'gallery2');
</script> 
</div>
    <br>
      <div class="more-results">
        <p class="more-results-text">For more results, please visit <a href="gallery.html", target="_blank">gallery!</a></p>
      </div>
    </div>
  </div>
</section>

<hr class="divider">

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="section-title has-text-centered is-centered">
      <h2 class="title is-3">From Doodles to Details: The Explainability of Sketch Synthesis</h2>
    </div>
      <div class="content has-text-justified"></div>
      As shown in the figure below, we demonstrate that our CLIPDrawX model offers the ability to synthesize explainable sketches whose strokes are primitive shapes like circles (second row from the top), straight lines (third row) and half circles (fourth row). These individual strokes can be tracked through their evolution in successive iterations of the optimization process. Notably, the model intuitively represents different parts of the synthesized sketch with appropriate primitive shapes. For example, the chassis and handlebars of the synthesized <em>motorcycle</em> are rendered with straight lines, while the wheels are depicted using circles and semicircles.<br>
      <br>Our model skilfully captures the dynamics of shape or scene evolution, displaying varying levels of flexibility based on the degree of freedom, which is linked to the number of control points in a shape.
      In the <em>farm</em> example, it assigns straight lines to simpler structures like a house's roof and walls, while more complex elements like grass and crops are made with semi-circles, providing more flexibility. Even more intricate structures, like trees, are rendered using circles, the most flexible shape, illustrating the model's skill in using various primitives for different levels of complexity. This strategic use of shapes enhances the model's ability to create detailed, nuanced sketches.
    </div><br>
    <div class="container is-max-desktop is-centered has-text-centered">

        
        <div class="captioned_video">
          <div class="inner_gallery2">
            <img class="gallery img-border" src="static/explain/motorcycle.png" alt="motorcycle" style="max-width: 100%;"/>
          </div>  
          <!-- <h6 class="caption">“A standing motorcycle”</h6> -->
          “A standing <mark>motorcycle</mark>”
        </div>

        <div class="captioned_video">
          <div class="inner_gallery2">
            <img class="gallery img-border" src="static/explain/farm.png" alt="farm" style="max-width: 100%;"/>
          </div>  
          “A peaceful afternoon at the <mark>farm</mark> sketch”
        </div>


      <div class="more-results">
        <p class="more-results-text">For more results, please visit <a href="explain.html", target="_blank">Explainable Sketch Synthesis!</a></p>
      </div>
    </div>
  </div>
</section>

<hr class="divider">
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="section-title has-text-centered is-centered">
      <h2 class="title is-3">Clearing the Clutter: Patch-based Initialization</h2>
    </div>
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls loop muted height="100%" style="max-width: 100%;">
        <!-- Your video here -->
        <source src="static/images/Patchification.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <div class="content has-text-justified">
      Placing different primitives at a single location is problematic, as it may result in clutter due to high point density, leading to uneven primitive distribution and messy sketches. To address this, our CLIPDrawX introduces primitives in fixed ranges or <em>patches</em>, representing all points within, rather than at precise attention map locations. It divides a <mark>\(224 \times 224\)</mark> canvas into patches of <mark>\(32 \times 32\)</mark>. 
        </div>
    </h2>
    </div>
  </div>
</section>


<hr class="divider">

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="section-title has-text-centered is-centered">
      <h2 class="title is-3">Fundamental Finesse: The Debut of Primitive-level Dropout</h2>
      <div class="content has-text-justified">
      Primitive-level dropout enhances the utilization of primitive shapes. 

      It facilitates the optimization process in a way that forces every primitive to independently encode some specific concept in the sketch. As a result, the number of noisy strokes in the sketch, that do not contribute towards depicting anything meaningful, is reduced.
      
      Moreover, PLD expedites convergence by reducing the number of steps required for structure creation, thanks to its ability to promptly detect relevant strokes.    <div class="container is-max-desktop is-centered has-text-centered">
      </div>
        <br>
        
        <div class="captioned_video">
            <img class="gallery" src="static/images/dropout_ablation.jpg" alt="dropout" style="width: 100%;"/>
        </div>

    </div>
  </div>
</section>

<hr class="divider">
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="section-title has-text-centered is-centered">
      <h2 class="title is-3">Gentle Strokes: Sketching with Diminished Opacity</h2>
      <div class="content has-text-justified">
      In traditional human sketching, artists often begin with a light outline or faint layout, serving as a foundation for the artwork. This initial phase sets the broader structure and composition. As the artwork advances, artists intensify strokes, especially focusing on crucial elements to make them prominent, ensuring each stroke adds value to the overall piece. Drawing a parallel to the digital realm, in CLIPDrawX, we have curated a similar methodology. Here, the initialization of primitive shapes starts with a low opacity value, denoted as \(\alpha\). This can be likened to the faint layout artists create. As the system begins its optimization process, based on relevance and significance, the opacity of certain primitives is incrementally increased. This mirrors the artist's method of iteratively intensifying strokes that are deemed crucial to the sketch's integrity.
      </div>
      <br>

        
        <div class="captioned_video">
            <img class="gallery" src="static/images/motorcycle.png" alt="opacity" style="width: 100%;"/>
          </div>
          “A standing <mark>motorcycle</mark>”

        <div class="captioned_video">
            <img class="gallery" src="static/images/missile.png" alt="opacity" style="width: 100%;"/>
          </div>
          “A <mark>missile</mark> ready to launch”


      </div>
    </div>
  </div>
</section>

<hr class="divider">

<section class="section hero">  
  <div class="container is-max-desktop">
    <div class="section-title has-text-centered is-centered">
      <h2 class="title is-3">Shades of Creativity: Exploring Sketch Variations</h2>

        
        <div class="captioned_video">
          <div class="inner_gallery2">
            <img class="gallery" src="static/images/variations.png" alt="var"/>
          </div>  
      </div>

    </div>
  </div>
</section>

<hr class="divider">

<section class="section hero">  
  <div class="container is-max-desktop">
    <div class="section-title has-text-centered is-centered">
      <h2 class="title is-3">How efficient is DiffuseKronA</h2>
      <div class="content has-text-justified">
        Owing to the intricate structure of the Kronecker adapter, it conducts a harmonious reduction in parameters along with the generation of high-fidelity images, a virtuoso performance that LoRA layers can only envy.
        The number of trainable parameters, as depicted in the table below, clearly indicates this. DiffuseKronA is \(\sim 35\%\) more parameter efficient as compared to LoRA-DreamBooth.
Furthermore, a reduced number of parameters results in a smaller fine-tuning module size, consequently lowering the overall storage requirements.
      </div>
      <table style="caption-side: bottom;">
        <caption>Comparison of LoRA-DreamBooth v.s. DiffuseKronA.</caption>
        <thead>
            <tr>
                <th>Backbone</th>
                <th>Model</th>
                <th>Train. Time</th>
                <th># Param</th>
                <th>Model size</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td rowspan="2" style="text-align: center; vertical-align: middle;">SDXL</td>
                <td>DiffuseKronA</td>
                <td>~ 40 min.</td>
                <td>3.8 M</td>
                <td>14.95 MB</td>
            </tr>
            <tr>
                <td>LoRA-DreamBooth</td>
                <td>~ 38 min.</td>
                <td>5.8 M</td>
                <td>22.32 MB</td>
            </tr>
            <tr>
                <td rowspan="2" style="text-align: center; vertical-align: middle;">SD</td>
                <td>DiffuseKronA</td>
                <td>~ 5.52 min.</td>
                <td>0.52 M</td>
                <td>2.1MB</td>
            </tr>
            <tr>
                <td>LoRA-Dreambooth</td>
                <td>~ 5.3 min.</td>
                <td>1.09 M</td>
                <td>4.3MB</td>
            </tr>
        </tbody>
    </table>
    </div>
  </div>
</section>

<hr class="divider">



<section class="section hero">  
  <div class="container is-max-desktop">
    <div class="section-title has-text-centered is-centered">
      <h2 class="title is-3">State-of-the-art Comparison</h2>
      <div class="content has-text-justified">
        We compare DiffuseKronA with four related methods, including \(\textbf{DreamBooth}\), \(\textbf{LoRA-DreamBooth}\), \(\textbf{\(\textbf{}\)}\), \(\textbf{SVDiff}\) and \(\textbf{SVDiff-LoRA}\). We maintain the original settings of all these methods to ensure a fair comparison. As shownbelow, our DiffuseKronA generates images that are highly aligned to the input images and constantly incorporates features mentioned in the input text prompt. The better fidelity and in-depth understanding of the input text prompts are attributed to the structure-preserving ability and better expressiveness of Kronecker product-based adaption. On the other hand, the images generated by LoRA-DreamBooth often require extensive fine-tuning to achieve the desired results. Methods like custom diffusion take more parameters to fine-tune the model.
      </div>
        <div class="captioned_video">
          <div class="inner_gallery2">
            <img class="gallery" src="static/images/comparison.png" alt="comp" style="width: 100%;"/>
          </div>  
          
      </div>
      <!-- <div class="more-results">
        <p class="more-results-text">For more results, please visit <a href="SOTA.html", target="_blank">SOTA Comparison!</a></p><br>
      </div>  -->
    </div>
  </div>
</section>

<hr class="divider">
<section class="section hero">  
  <div class="container is-max-desktop">
    <div class="section-title has-text-centered is-centered">
      <h2 class="title is-3">Final takeaways</h2><br>
      <div class="content has-text-justified">
        We proposed a new parameter-efficient adaption module, DiffuseKronA, to enhance text-to-image personalized diffusion models, aiming to achieve high-quality image generation with improved parameter efficiency. Leveraging the Kronecker product's capacity to capture structured relationships in weight matrices, DiffuseKronA produces images closely aligned with input text prompts and training images, outperforming LoRA-DreamBooth in visual quality, text alignment, fidelity, parameter efficiency, and stability.
        Our method thus provides a new and efficient tool for advancing text-to-image personalized image generation tasks.
      </div>
    </div>
  </div>
</section>

<hr class="divider">


<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <button class="copy-button">Copy</button>
    <h2 class="title">BibTeX</h2>
    <pre><code id="bibtex">
      @misc{marjit2023diffusekrona,
        title={DiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized Diffusion Model}, 
        author={Shyam Marjit and Harshit Singh and Nityanand Mathur and Sayak Paul and Chia-Mu Yu and Pin-Yu Chen},
        year={2023},
        eprint={},
        archivePrefix={arXiv},
        primaryClass={cs.CV}
      }
    </code></pre>
  </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
           Page credits: <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

  </body>
  </html>
